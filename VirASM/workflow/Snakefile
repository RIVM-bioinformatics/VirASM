import pprint
import yaml
import os
import sys
import json
from directories import *
import snakemake

snakemake.utils.min_version("6.0")

yaml.warnings({'YAMLLoadWarning': False})
shell.executable('/bin/bash')

SAMPLES = {}

with open("samplesheet.yaml") as sheetfile:
    SAMPLES = yaml.safe_load(sheetfile)

def low_memory_job(wildcards, threads, attempt):
    if config['computing_execution'] == 'local':
        return min(attempt * threads * 1 * 1000, config['max_local_mem'])
    return attempt * threads * 1 * 1000

def medium_memory_job(wildcards, threads, attempt):
    if config['computing_execution'] == 'local':
        return min(attempt * threads * 2 * 1000, config['max_local_mem'])
    return attempt * threads * 2 * 1000

def high_memory_job(wildcards, threads, attempt):
    if config['computing_execution'] == 'local':
        return min(attempt * threads * 4 * 1000, config['max_local_mem'])
    return attempt * threads * 4 * 1000

def very_high_memory_job(wildcards, threads, attempt):
    if config['computing_execution'] == 'local':
        return min(attempt * threads * 4 * 1.75 * 1000, config['max_local_mem'])
    return attempt * threads * 4 * 1.75 * 1000


rule all:
    input:
        f"{res}multiqc.html",
        expand("{p}{sample}_scaffolds.fasta", 
            p = f"{res+scf}",
            sample = SAMPLES)


rule QC_raw:
    input: lambda wildcards: SAMPLES[wildcards.sample][wildcards.read]
    output:
        html = f"{datadir + qc_pre}" + "{sample}_{read}_fastqc.html",
        zipf = f"{datadir + qc_pre}" + "{sample}_{read}_fastqc.zip" 
    conda:
        f"{conda_envs}Clean.yaml"
    log:
        f"{logdir}" + "QC_raw_{sample}_{read}.log"
    benchmark:
        f"{logdir + bench}" + "QC_raw_{sample}_{read}.txt"
    threads: config['threads']['Filter']
    resources:
        mem_mb = low_memory_job
    params:
        output_dir = f"{datadir + qc_pre}",
        script = srcdir("scripts/fqc.sh")
    shell:
        """
        bash {params.script} {input} {params.output_dir} {output.html} {output.zipf} {log} {threads}
        """

rule QC_filter:
    input: lambda wildcards: (SAMPLES[wildcards.sample][i] for i in ("R1", "R2"))
    output: 
        r1 = f"{datadir + cln + qcfilt}" + "{sample}_R1.fq",
        r2 = f"{datadir + cln + qcfilt}" + "{sample}_R2.fq",
        unpaired = f"{datadir + cln + qcfilt}" + "{sample}_U.fq",
        html = f"{datadir + cln + qcfilt + html}" + "{sample}.fastp.html",
        json = f"{datadir + cln + qcfilt + json}" + "{sample}.fastp.json",
    conda:
        f"{conda_envs}Clean.yaml"
    log:
        f"{logdir}" + "QC_filter_{sample}.log"
    benchmark:
        f"{logdir + bench}" + "QC_filter_{sample}.txt"
    threads: config['threads']['Filter']
    resources:
        mem_mb = medium_memory_job
    params:
        score = config['runparams']['qc_filter_score'],
        size = config['runparams']['qc_filter_size'],
        length= config['runparams']['qc_filter_length'],
        adapters = srcdir("files/nexteraPE_adapters.fa")
    shell:
        """
        fastp --thread {threads} \
        --in1 {input[0]:q} \
        --in2 {input[1]:q} \
        --out1 {output.r1} \
        --out2 {output.r2} \
        --unpaired1 {output.unpaired} \
        --unpaired2 {output.unpaired} \
        -h {output.html} \
        -j {output.json} \
        -q {params.score} \
        --cut_right --cut_right_window_size {params.size} \
        --cut_right_mean_quality {params.score} \
        --length_required {params.length} \
        --adapter_fasta {params.adapters} > {log} 2>&1
        """ 

rule QC_clean:
    input: 
        f"{datadir + cln + qcfilt}" + "{sample}_{read}.fq"
    output:
        html = f"{datadir + qc_post}" + "{sample}_{read}_fastqc.html",
        zipf = f"{datadir + qc_post}" + "{sample}_{read}_fastqc.zip"
    conda:
        f"{conda_envs}Clean.yaml"
    log:
        f"{logdir}" + "QC_clean_{sample}_{read}.log"
    benchmark:
        f"{logdir + bench}" + "QC_clean_{sample}_{read}.txt"
    threads: config['threads']['Filter']
    resources:
        mem_mb = low_memory_job
    params:
        output_dir = f"{datadir + qc_post}"
    shell:
        """
        if [ -s {input} ]; then
            fastqc -t {threads} --quiet --outdir {params.output_dir} {input} > {log} 2>&1
        else
            touch {output.html}
            touch {output.zipf}
            echo "touched things because input was empty" > {log} 2>&1
        fi
        """

rule Remove_BG_p1:
    input: 
        bg = config['db']['background'],
        r1 = rules.QC_filter.output.r1,
        r2 = rules.QC_filter.output.r2,
        un = rules.QC_filter.output.unpaired
    output: 
        bam = f"{datadir + cln + aln}" + "{sample}_raw-alignment.bam",
        bai = f"{datadir + cln + aln}" + "{sample}_raw-alignment.bam.bai"
    conda:
        f"{conda_envs}Clean.yaml"
    log:
        f"{logdir}" + "Remove_BG_p1_{sample}.log"
    benchmark:
        f"{logdir + bench}" + "Remove_BG_p1_{sample}.txt"
    threads: config['threads']['Alignments']
    resources:
        mem_mb = high_memory_job
    params:
        aln_type = '--local'
    shell:
        """
        bowtie2 --time --threads {threads} \
        {params.aln_type} \
        -x {input.bg} \
        -1 {input.r1} \
        -2 {input.r2} \
        -U {input.un} 2> {log} |\
        samtools view -@ {threads} -uS - 2>> {log} |\
        samtools sort -@ {threads} - -o {output.bam} >> {log} 2>&1
        samtools index -@ {threads} {output.bam} >> {log} 2>&1
        """


rule Remove_BG_p2:
    input: 
        bam = rules.Remove_BG_p1.output.bam,
        bai = rules.Remove_BG_p1.output.bai
    output: 
        r1 = f"{datadir + cln + filt}" + "{sample}_pR1.fq",
        r2 = f"{datadir + cln + filt}" + "{sample}_pR2.fq",
    conda:
        f"{conda_envs}Clean.yaml"
    log:
        f"{logdir}" + "Remove_BG_p2_{sample}.log"
    benchmark:
        f"{logdir + bench}" + "Remove_BG_p2_{sample}.txt"
    threads: config['threads']['Filter']
    resources:
        mem_mb = medium_memory_job
    shell:
        """
        samtools view -@ {threads} -b -f 1 -f 8 {input.bam} 2> {log} |\
        samtools sort -@ {threads} -n - 2>> {log} |\
        bedtools bamtofastq -i - -fq {output.r1} -fq2 {output.r2} >> {log} 2>&1
        """

rule Remove_BG_p3:
    input: 
        bam = rules.Remove_BG_p1.output.bam,
        bai = rules.Remove_BG_p1.output.bai    
    output: 
        tbam = temp(f"{datadir + cln + aln}" + "{sample}_temp_unpaired.bam"),
        un = f"{datadir + cln + filt}" + "{sample}_unpaired.fq",
    conda:
        f"{conda_envs}Clean.yaml"
    log:
        f"{logdir}" + "Remove_BG_p3_{sample}.log"
    benchmark:
        f"{logdir + bench}" + "Remove_BG_p3_{sample}.txt"
    threads: config['threads']['Filter']
    resources:
        mem_mb = medium_memory_job
    shell:
        """
        samtools view -@ {threads} -b -F 1 -f 4 {input.bam} 2> {log} |\
        samtools sort -@ {threads} -n -o {output.tbam} 2>> {log}
        bedtools bamtofastq -i {output.tbam} -fq {output.un} >> {log} 2>&1
        """

rule Assemble:
    input: 
        r1 = rules.Remove_BG_p2.output.r1,
        r2 = rules.Remove_BG_p2.output.r2,
        un = rules.Remove_BG_p3.output.un
    output: 
        scaffolds = f"{datadir + asm + raw}" + "{sample}/scaffolds.fasta",
        scaff_filt = f"{datadir + asm + filt}" + "{sample}_scaffolds_filtered.fasta",
    conda:
        f"{conda_envs}Assemble.yaml"
    log:
        f"{logdir}" + "Assemble_{sample}.log"
    benchmark:
        f"{logdir + bench}" + "Assemble_{sample}.txt"
    threads: config['threads']['Assemble']
    resources:
        mem_mb = very_high_memory_job
    params:
        min_contig_len = config['Assembly']['min_contig_len'],
        kmersizes = config['Assembly']['kmersizes'],
        outdir = f"{datadir + asm + raw}" + "{sample}/"
    shell:
        """
        spades.py --only-assembler --meta \
        -1 {input.r1} \
        -2 {input.r2} \
        -s {input.un} \
        -t {threads} \
        -m $(({resources.mem_mb} / 1000)) \
        -k {params.kmersizes} \
        -o {params.outdir} > {log} 2>&1
        seqtk seq {output.scaffolds} 2>> {log} |\
        gawk -F "_" '/^>/ {{if ($4 >= {params.min_contig_len}) {{print $0; getline; print $0}};}}' 2>> {log} 1> {output.scaff_filt} 
        """

rule Copy_scaffolds:
    input: rules.Assemble.output.scaff_filt
    output: f"{res+scf}" + "{sample}_scaffolds.fasta"
    threads: 1
    resources:
        mem_mb = low_memory_job
    shell:
        """
        cp {input} {output}
        """	

rule MultiQC:
    input: 
        expand( rules.QC_raw.output.zipf,
                sample = SAMPLES,
                read = ['R1', 'R2'] ),
        expand( rules.QC_clean.output.zipf,
                sample = SAMPLES,
                read = ['R1', 'R2'] ),
        expand( rules.QC_filter.output.json,
                sample = SAMPLES,
                read = ['R1', 'R2', 'U'] ),
        expand( rules.Remove_BG_p1.log,
                sample = SAMPLES ),
    output: 
        f"{res}multiqc.html",
        expand( "{p}multiqc_{program}.txt",
                p = f"{res+mqc_data}",
                program = ['fastqc', 'fastp', 'bowtie2'] ),
    conda:
        f"{conda_envs}Clean.yaml"
    log:
        f"{logdir}" + "MultiQC.log"
    benchmark:
        f"{logdir + bench}" + "MultiQC.txt"
    threads: config['threads']['MultiQC']
    resources:
        mem_mb = medium_memory_job
    params:
        conf = srcdir("files/multiqc_config.yaml"),
        outdir = f"{res}"
    shell:
        """
        multiqc -d --force --config {params.conf} -o {params.outdir} -n multiqc.html {input} > {log} 2>&1
        """


onsuccess:
    print("""
    VirASM is finished with processing all the files in the given input directory.

    Shutting down...
    """)
    return True

onerror:
    print("""
    An error occurred and VirASM had to shut down.
    Please check the the input and lugfiles for any abnormalities and try again.
    """)
    return False